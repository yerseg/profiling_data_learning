{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, plot_roc_curve, make_scorer\n",
    "from sklearn import preprocessing\n",
    "from scipy import stats\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate, LeaveOneGroupOut, PredefinedSplit, GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframe_impl(df_count, rolling=True):\n",
    "    dfs_list = []\n",
    "    dfs_rows_len_list = []\n",
    "\n",
    "    file_name = \"\"\n",
    "    if rolling is True:\n",
    "        file_name = \".\\\\_datasets\\\\600s\\\\location_rolling_dataset_\"\n",
    "    else:\n",
    "        file_name = \".\\\\_datasets\\\\600s\\\\location_sampling_dataset_\"\n",
    "\n",
    "    for i in range(1, df_count + 1):\n",
    "        df = pd.read_csv(file_name + str(i) + \".csv\")\n",
    "        df = df.drop([\"timestamp\"], axis=1)\n",
    "\n",
    "        df = (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "        df[\"user\"] = i\n",
    "        dfs_list.append(df)\n",
    "        dfs_rows_len_list.append(df.shape[0])\n",
    "\n",
    "    df = pd.concat(dfs_list, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_common_rolling_dataframe(df_count):\n",
    "    return make_dataframe_impl(df_count, True)\n",
    "\n",
    "def make_common_sampling_dataframe(df_count):\n",
    "    return make_dataframe_impl(df_count, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_bad_rows(df):\n",
    "    bad_rows = set()\n",
    "    for col in df.columns:\n",
    "        if col != \"user\":\n",
    "            for user in df.user.unique():\n",
    "                for x in list(\n",
    "                    df.loc[df.user == user, :][\n",
    "                        np.abs(stats.zscore(df.loc[df.user == user, col])) > 2\n",
    "                    ].index\n",
    "                ):\n",
    "                    bad_rows.add(x)\n",
    "\n",
    "            for x in list(df[col][np.abs(stats.zscore(df[col])) > 2].index):\n",
    "                bad_rows.add(x)\n",
    "\n",
    "    df.drop(list(bad_rows), axis=0, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def drop_bad_cols(df):\n",
    "    bad_cols = set()\n",
    "    for col in df.columns:\n",
    "        if col != \"user\":\n",
    "            if df[df[col] != df[col].mean()].shape[0] < 0.1 * df.shape[0]:\n",
    "                bad_cols.add(col)\n",
    "\n",
    "            for user in df.user.unique():\n",
    "                if (\n",
    "                    df.loc[df.user == user, :][\n",
    "                        df.loc[df.user == user, col]\n",
    "                        != df.loc[df.user == user, col].mean()\n",
    "                    ].shape[0]\n",
    "                    < 0.1 * df.loc[df.user == user, :].shape[0]\n",
    "                ):\n",
    "                    bad_cols.add(col)\n",
    "\n",
    "                elif (\n",
    "                    np.sum(np.abs(stats.zscore(df.loc[df.user == user, col])) < 2)\n",
    "                    < 0.9 * df.loc[df.user == user, col].shape[0]\n",
    "                ):\n",
    "                    bad_cols.add(col)\n",
    "\n",
    "    df.drop(list(bad_cols), axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(df):\n",
    "    sampling_dfs = []\n",
    "    need_count = 0\n",
    "\n",
    "    for label, count in zip(df.user.value_counts().index, df.user.value_counts().values):\n",
    "        if need_count == 0:\n",
    "            need_count = count\n",
    "            df_ = df[df.user == label]\n",
    "            sampling_dfs.append(df_)\n",
    "        else:\n",
    "            df_ = df[df.user == label]\n",
    "            df_over = df_.sample(need_count, replace=True, random_state=42)\n",
    "            sampling_dfs.append(df_over)\n",
    "\n",
    "    new_df = pd.concat(sampling_dfs)\n",
    "    new_df = new_df.reset_index()\n",
    "\n",
    "    return pd.concat(sampling_dfs)\n",
    "\n",
    "def extract_delayed_user(df, user_label):\n",
    "    df_user = df[df[\"user\"] == user_label]\n",
    "    df = df[df[\"user\"] != user_label]\n",
    "    return df_user, df\n",
    "\n",
    "def split_users_into_two_classes(df, valid_user_label):\n",
    "    df.loc[df[\"user\"] != valid_user_label, \"user\"] = 0\n",
    "    df.loc[df[\"user\"] == valid_user_label, \"user\"] = 1\n",
    "    return df\n",
    "\n",
    "def get_cv_split(X, y, group_labels, valid_user_label):\n",
    "    predefined_split_array = np.zeros(group_labels.shape[0])\n",
    "    i = 0\n",
    "    test_array = [x for x in range(group_labels.shape[0])]\n",
    "    for test, _ in LeaveOneGroupOut().split(X, y, group_labels):\n",
    "        diff = np.setdiff1d(test_array, test)\n",
    "        if np.all(group_labels[diff[0] : diff[-1]] == valid_user_label) is np.bool_(True):\n",
    "            for sample in diff:\n",
    "                predefined_split_array[sample] = -1\n",
    "        else:\n",
    "            for sample in diff:\n",
    "                predefined_split_array[sample] = i\n",
    "            i += 1\n",
    "    return predefined_split_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_common_rolling_dataframe(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accuracy_mean',\n",
       " 'accuracy_var',\n",
       " 'accuracy_median',\n",
       " 'accuracy_skew',\n",
       " 'accuracy_kurt',\n",
       " 'accuracy_std',\n",
       " 'speed_mean',\n",
       " 'speed_var',\n",
       " 'speed_median',\n",
       " 'speed_skew',\n",
       " 'speed_kurt',\n",
       " 'speed_std',\n",
       " 'altitude_speed_mean',\n",
       " 'altitude_speed_var',\n",
       " 'altitude_speed_median',\n",
       " 'altitude_speed_skew',\n",
       " 'altitude_speed_kurt',\n",
       " 'altitude_speed_std',\n",
       " 'acc_mean',\n",
       " 'acc_var',\n",
       " 'acc_median',\n",
       " 'acc_skew',\n",
       " 'acc_kurt',\n",
       " 'acc_std',\n",
       " 'altitude_acc_mean',\n",
       " 'altitude_acc_var',\n",
       " 'altitude_acc_median',\n",
       " 'altitude_acc_skew',\n",
       " 'altitude_acc_kurt',\n",
       " 'altitude_acc_std',\n",
       " 'user']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"accuracy_mean\",\n",
    "    \"accuracy_var\",\n",
    "    \"accuracy_median\",\n",
    "    \"accuracy_skew\",\n",
    "    \"accuracy_kurt\",\n",
    "    \"accuracy_std\",\n",
    "    \"speed_mean\",\n",
    "    \"speed_var\",\n",
    "    \"speed_median\",\n",
    "    #  'speed_skew', - переобучается и у нас получаются 0 аккураси\n",
    "    \"speed_kurt\",\n",
    "    \"speed_std\",\n",
    "    \"altitude_speed_mean\",\n",
    "    \"altitude_speed_var\",\n",
    "    \"altitude_speed_median\",\n",
    "    #  'altitude_speed_skew', - сильное переобучение, каждое второе - либо 1 либо 0\n",
    "    \"altitude_speed_kurt\",\n",
    "    \"altitude_speed_std\",\n",
    "    \"acc_mean\",\n",
    "    \"acc_var\",\n",
    "    #  'acc_median',\n",
    "    #  'acc_skew',\n",
    "    \"acc_kurt\",\n",
    "    \"acc_std\",\n",
    "    #  'altitude_acc_mean',\n",
    "    \"altitude_acc_var\",\n",
    "    \"altitude_acc_median\",\n",
    "    #  'altitude_acc_skew',\n",
    "    \"altitude_acc_kurt\",\n",
    "    \"altitude_acc_std\",\n",
    "    \"user\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.columns.difference(features), axis=1)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_bad_cols(df)\n",
    "df = drop_bad_rows(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "corr_cols = [column for column in upper_tri.columns if any(abs(upper_tri[column]) > 0.7) and column != \"user\"]\n",
    "df = df.drop(corr_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'] = df['user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_df_parts = []\n",
    "for user in df.labels.unique():\n",
    "    new_df = df[df.labels == user].sample(int(df[df.labels == user].shape[0] * 0.2)).copy()\n",
    "    gs_df_parts.append(new_df)\n",
    "\n",
    "df = pd.concat(gs_df_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoostClassifier GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in df.labels.unique():\n",
    "    print(\"Valid User: \", user)\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "    df_ = resample(df.copy())\n",
    "    df_ = split_users_into_two_classes(df_, user)\n",
    "    df_ = resample(df_)\n",
    "\n",
    "    group_labels = df_.labels.to_numpy().copy()\n",
    "    df_ = df_.drop('labels', axis=1)\n",
    "\n",
    "    dataset = df_.to_numpy().copy()\n",
    "    X = dataset[:, :-1]\n",
    "    y = dataset[:, -1]\n",
    "\n",
    "    cv_split = PredefinedSplit(test_fold=get_cv_split(X, y, group_labels, user))\n",
    "\n",
    "    clf = CatBoostClassifier()\n",
    "    params = {'iterations': [100, 500],\n",
    "              'depth': [6, 10],\n",
    "              'loss_function': ['Logloss', 'CrossEntropy'],\n",
    "              'l2_leaf_reg': [1, 10, 100],\n",
    "              'leaf_estimation_iterations': [2, 5, 10],\n",
    "              'logging_level':['Silent']\n",
    "              }\n",
    "\n",
    "    scorer = make_scorer(accuracy_score)\n",
    "    clf_grid = GridSearchCV(estimator=clf, param_grid=params, scoring=scorer, cv=cv_split)\n",
    "\n",
    "    clf_grid.fit(X, y)\n",
    "\n",
    "    best_params = clf_grid.best_params_\n",
    "    print('Best params: ', best_params)\n",
    "\n",
    "    print(\"--------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestClassifier GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in df.labels.unique():\n",
    "    print(\"Valid User: \", user)\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "    df_ = resample(df.copy())\n",
    "    df_ = split_users_into_two_classes(df_, user)\n",
    "    df_ = resample(df_)\n",
    "\n",
    "    group_labels = df_.labels.to_numpy().copy()\n",
    "    df_ = df_.drop('labels', axis=1)\n",
    "\n",
    "    dataset = df_.to_numpy().copy()\n",
    "    X = dataset[:, :-1]\n",
    "    y = dataset[:, -1]\n",
    "\n",
    "    cv_split = PredefinedSplit(test_fold=get_cv_split(X, y, group_labels, user))\n",
    "\n",
    "    clf = RandomForestClassifier()\n",
    "    params = {'n_estimators': [50, 100],\n",
    "              'criterion': ['gini', 'entropy'],\n",
    "              'max_depth': [10, 100, None],\n",
    "              'min_samples_split': [2, 100],\n",
    "              'min_samples_leaf': [1, 10, 100],\n",
    "              'max_features':['auto', 'sqrt', 'log2'],\n",
    "              'n_jobs': [-1],\n",
    "              'class_weight': ['balanced']\n",
    "             }\n",
    "\n",
    "    scorer = make_scorer(accuracy_score)\n",
    "    clf_grid = GridSearchCV(estimator=clf, param_grid=params, scoring=scorer, cv=cv_split)\n",
    "\n",
    "    clf_grid.fit(X, y)\n",
    "\n",
    "    best_params = clf_grid.best_params_\n",
    "    print('Best params: ', best_params)\n",
    "\n",
    "    print(\"--------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in df.labels.unique():\n",
    "    print(\"Valid User: \", user)\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "    df_ = resample(df.copy())\n",
    "    df_ = split_users_into_two_classes(df_, user)\n",
    "    df_ = resample(df_)\n",
    "    \n",
    "    df_.loc[df_.user == 0, 'user'] = -1\n",
    "\n",
    "    group_labels = df_.labels.to_numpy().copy()\n",
    "    df_ = df_.drop('labels', axis=1)\n",
    "\n",
    "    dataset = df_.to_numpy().copy()\n",
    "    X = dataset[:, :-1]\n",
    "    y = dataset[:, -1]\n",
    "\n",
    "    cv_split = PredefinedSplit(test_fold=get_cv_split(X, y, group_labels, user))\n",
    "\n",
    "    clf = SVC()\n",
    "    params = {'C': [0.01, 1, 10, 100],\n",
    "              'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "              'degree': [1, 2, 3, 4],\n",
    "              'gamma': ['scale', 'auto', 0.01, 0.1, 1, 3, 5],\n",
    "              'cache_size': [2000],\n",
    "              'max_iter':[-1, 100, 1000]\n",
    "             }\n",
    "\n",
    "    scorer = make_scorer(accuracy_score)\n",
    "    clf_grid = GridSearchCV(estimator=clf, param_grid=params, scoring=scorer, cv=cv_split)\n",
    "\n",
    "    clf_grid.fit(X, y)\n",
    "\n",
    "    best_params = clf_grid.best_params_\n",
    "    print('Best params: ', best_params)\n",
    "\n",
    "    print(\"--------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogReg GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in df.labels.unique():\n",
    "    print(\"Valid User: \", user)\n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "    df_ = resample(df.copy())\n",
    "    df_ = split_users_into_two_classes(df_, user)\n",
    "    df_ = resample(df_)\n",
    "\n",
    "    group_labels = df_.labels.to_numpy().copy()\n",
    "    df_ = df_.drop('labels', axis=1)\n",
    "\n",
    "    dataset = df_.to_numpy().copy()\n",
    "    X = dataset[:, :-1]\n",
    "    y = dataset[:, -1]\n",
    "\n",
    "    cv_split = PredefinedSplit(test_fold=get_cv_split(X, y, group_labels, user))\n",
    "\n",
    "    clf = LogisticRegression()\n",
    "    params = {'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "              'C': [0.01, 1, 100],\n",
    "              'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "              'max_iter': [500, 1000, 10000],\n",
    "              'n_jobs': [-1]\n",
    "             }\n",
    "\n",
    "    scorer = make_scorer(accuracy_score)\n",
    "    clf_grid = GridSearchCV(estimator=clf, param_grid=params, scoring=scorer, cv=cv_split)\n",
    "\n",
    "    clf_grid.fit(X, y)\n",
    "\n",
    "    best_params = clf_grid.best_params_\n",
    "    print('Best params: ', best_params)\n",
    "\n",
    "    print(\"--------------------------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
