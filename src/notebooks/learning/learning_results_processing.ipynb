{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, plot_roc_curve, make_scorer, f1_score, roc_auc_score, det_curve\n",
    "from sklearn import preprocessing\n",
    "from scipy import stats\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate, LeaveOneGroupOut, PredefinedSplit, GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "import json\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(d, u):\n",
    "    import collections.abc\n",
    "    for k, v in u.items():\n",
    "        if isinstance(v, collections.abc.Mapping):\n",
    "            d[k] = update(d.get(k, {}), v)\n",
    "        else:\n",
    "            d[k] = v\n",
    "    return d\n",
    "\n",
    "\n",
    "def update_file_with_results(file_path, results_dict):\n",
    "    with open(file_path, 'r') as f:\n",
    "        res = json.load(f)\n",
    "    \n",
    "    res = update_dict(res, results_dict)\n",
    "    \n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(res, f, sort_keys=True, indent=2)\n",
    "        \n",
    "        \n",
    "def get_dict_with_results(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        res = json.load(f)\n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eer(fpr, fnr, thresholds):\n",
    "    idx = np.nanargmin(np.absolute((fnr - fpr)))\n",
    "    eer_threshold = thresholds[idx]\n",
    "    eer1 = fpr[idx]\n",
    "    \n",
    "    return eer1, eer_threshold\n",
    "\n",
    "\n",
    "def auc_det(fpr, fnr):\n",
    "    return metrics.auc(fpr, fnr)\n",
    "\n",
    "\n",
    "def auc_roc(fpr, tpr):\n",
    "    return metrics.auc(fpr, tpr)\n",
    "\n",
    "\n",
    "def confusion_matrix_thr(y_true, proba, threshold):\n",
    "    predict = proba\n",
    "    predict[predict > threshold] = 1\n",
    "    predict[predict <= threshold] = 0\n",
    "    \n",
    "    matr = metrics.confusion_matrix(y_true, predict, labels=[0, 1])\n",
    "    \n",
    "    tp = matr[0, 0]\n",
    "    fp = matr[1, 0]\n",
    "    fn = matr[0, 1]\n",
    "    tn = matr[1, 1]\n",
    "    \n",
    "    return tn, fp, fn, tp\n",
    "\n",
    "\n",
    "def calc_metrics(y_test, proba, thresholds):\n",
    "    FPR = np.array([])\n",
    "    TPR = np.array([])\n",
    "    FNR = np.array([])\n",
    "    F_score = np.array([])\n",
    "    ANGA = np.array([])\n",
    "    ANIA = np.array([])\n",
    "\n",
    "    for thr in thresholds:\n",
    "        tn, fp, fn, tp = confusion_matrix_thr(y_test, proba.copy(), thr)\n",
    "        \n",
    "        fpr = fp / (tn + fp)\n",
    "        tpr = tp / (tp + fn)\n",
    "        fnr = fn / (tp + fn)\n",
    "        \n",
    "        FPR = np.append(FPR, 1 if np.isnan(fpr) else fpr)\n",
    "        TPR = np.append(TPR, 1 if np.isnan(tpr) else tpr)\n",
    "        FNR = np.append(FNR, 1 if np.isnan(fnr) else fnr)\n",
    "        F_score = np.append(F_score, tp / (tp + 0.5 * (fn + fp)))\n",
    "    \n",
    "    EER, EER_thr = eer(fpr=FPR, fnr=FNR, thresholds=thresholds)\n",
    "    AUC_DET = auc_det(fpr=FPR, fnr=FNR)\n",
    "    AUC_ROC = auc_roc(fpr=FPR, tpr=TPR)\n",
    "    \n",
    "    return {'FAR': FPR, \n",
    "            'FRR': FNR, \n",
    "            'F': F_score, \n",
    "            'EER': EER, \n",
    "            'EER_thr': EER_thr, \n",
    "            'AUC-DET': AUC_DET, \n",
    "            'AUC-ROC': AUC_ROC}\n",
    "\n",
    "\n",
    "def iterate_over_cv_results(results):\n",
    "    for df_type, inner in results.items():\n",
    "        if df_type == 'stub':\n",
    "            continue\n",
    "\n",
    "        for window_type, inner1 in inner.items():\n",
    "            for window_size, inner2 in inner1.items():\n",
    "                for model, inner3 in inner2.items():\n",
    "                    for valid_user, inner4 in inner3['cross_validation']['valid_user'].items():\n",
    "                        yield {'df_type': df_type, \n",
    "                               'window_type': window_type, \n",
    "                               'window_size': window_size, \n",
    "                               'model': model, \n",
    "                               'valid_user': valid_user, \n",
    "                               'accuracy': np.array(inner7['accuracy'])}\n",
    "\n",
    "                        \n",
    "def iterate_over_final_results(results):\n",
    "    for df_type, inner in results.items():\n",
    "        if df_type == 'stub':\n",
    "            continue\n",
    "\n",
    "        for window_type, inner1 in inner.items():\n",
    "            for window_size, inner2 in inner1.items():\n",
    "                for model, inner3 in inner2.items():\n",
    "                    for valid_user, inner4 in inner3['final_validation']['valid_user'].items():\n",
    "                        for intruder, inner5 in inner4['extracted_user'].items(): \n",
    "                            yield {'df_type': df_type, \n",
    "                                   'window_type': window_type, \n",
    "                                   'window_size': window_size, \n",
    "                                   'model': model, \n",
    "                                   'valid_user': valid_user, \n",
    "                                   'intruder': intruder,\n",
    "                                   'test': np.array(inner5['test']), \n",
    "                                   'proba': np.array(inner5['proba'])[:, 1]}\n",
    "            \n",
    "\n",
    "def avg_accuracy(results):\n",
    "    metrics = {}\n",
    "    for res in iterate_over_cv_results(results):\n",
    "        key = (res['df_type'], res['window_type'], res['window_size'], res['model'])\n",
    "        if key not in metrics.keys():\n",
    "            metrics[key] = {'accuracy': []}\n",
    "        \n",
    "        metrics[key]['accuracy'].append(res['accuracy'])\n",
    "        \n",
    "    for k, v in metrics.items():\n",
    "        metrics[k] = ({'accuracy': np.array(v['accuracy']).mean()})    \n",
    "        \n",
    "    return metrics\n",
    "          \n",
    "    \n",
    "def avg_common_metrics(results, thresholds):\n",
    "    metrics = {}\n",
    "    for res in iterate_over_results(results):\n",
    "        key = (res['df_type'], res['window_type'], res['window_size'], res['model'])\n",
    "        if key not in metrics.keys():\n",
    "            metrics[key] = {'EER': [], 'AUC-DET': [], 'AUC-ROC': []}\n",
    "        \n",
    "        metrics_dict = calc_metrics(res['test'], res['proba'], thresholds)\n",
    "        \n",
    "        metrics[key]['EER'].append(metrics_dict['EER'])\n",
    "        metrics[key]['AUC-DET'].append(metrics_dict['AUC-DET'])\n",
    "        metrics[key]['AUC-ROC'].append(metrics_dict['AUC-ROC'])\n",
    "        \n",
    "    for k, v in metrics.items():\n",
    "        metrics[k] = ({'EER': np.array(v['EER']).mean(), \n",
    "                       'AUC-DET': np.array(v['AUC-DET']).mean(), \n",
    "                       'AUC-ROC': np.array(v['AUC-ROC']).mean()})\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda3\\lib\\site-packages\\docx\\styles\\styles.py:139: UserWarning: style lookup by style_id is deprecated. Use style name as key instead.\n",
      "  return self._get_style_id_from_style(self[style_name], style_type)\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "from docx.shared import Cm, Pt\n",
    "\n",
    "def add_columns_names(table, names, row_index = 0):\n",
    "    for name, i in zip(names, range(len(names))):\n",
    "        table.rows[row_index].cells[i].text = str(name)\n",
    "    return table\n",
    "\n",
    "\n",
    "def add_rows_names(table, names, col_index = 0):\n",
    "    for name, i in zip(names, range(len(names))):\n",
    "        table.rows[i].cells[col_index].text = str(name)\n",
    "    return table\n",
    "\n",
    "word_document = Document()\n",
    "document_name = 'test'\n",
    "\n",
    "values = [[1,2,3], [4,5,6], [7,8,9]]\n",
    "    \n",
    "# customizing the table\n",
    "table = word_document.add_table(rows=10, cols=6) # we add rows iteratively\n",
    "table.style = 'TableGrid'\n",
    "\n",
    "NameIdx = 0\n",
    "WndIdx = 1\n",
    "CatBoostIdx = 2\n",
    "RandomForestIdx = 3\n",
    "SVCIdx = 4\n",
    "LogRegIdx = 5\n",
    "\n",
    "table = add_columns_names(table, ['Метрика', 'Размер окна, с', 'CatBoostClassifier', 'RandomForest', 'SVM-SVC', 'LogisticRegression'])\n",
    "table = add_columns_names(table, ['Метрика', 'Accuracy'])\n",
    "\n",
    "for index, stat_item in enumerate(text_stats.items()):\n",
    "    stat_name, stat_result = stat_item\n",
    "    row = table.rows[index]\n",
    "    row.cells[0].text = str(stat_name)\n",
    "    row.cells[1].text = str(stat_result)\n",
    "    \n",
    "word_document.add_page_break()\n",
    "\n",
    "word_document.save(document_name + '.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_common_accuracy_tables(results, df_type, window_type, window_sizes):\n",
    "    word_document = Document()\n",
    "    document_name = '_'.join(['df_type', 'window_type'])\n",
    "    \n",
    "    table = word_document.add_table(rows=10, cols=6) # we add rows iteratively\n",
    "    table.style = 'TableGrid'\n",
    "    \n",
    "    NameIdx = 0\n",
    "    WndIdx = 1\n",
    "    CatBoostIdx = 2\n",
    "    RandomForestIdx = 3\n",
    "    SVCIdx = 4\n",
    "    LogRegIdx = 5\n",
    "    \n",
    "    def get_col_idx(model_tag):\n",
    "        if model_tag == 'CatBoost':\n",
    "            return CatBoostIdx\n",
    "        if model_tag == 'RandomForest':\n",
    "            return RandomForestIdx\n",
    "        if model_tag == 'SVC':\n",
    "            return SVCIdx\n",
    "        if model_tag == 'LogReg':\n",
    "            return LogRegIdx\n",
    "    \n",
    "    s5 = 1\n",
    "    s10 = 2\n",
    "    s30 = 3\n",
    "    s60 = 4\n",
    "    s90 = 5\n",
    "    s120 = 6\n",
    "    s240 = 7\n",
    "    s600 = 8\n",
    "    \n",
    "    def get_row_idx(wnd):\n",
    "        if wnd == '5s':\n",
    "            return s5\n",
    "        if wnd == '10s':\n",
    "            return s10\n",
    "        if wnd == '30s':\n",
    "            return s30\n",
    "        if wnd == '60s':\n",
    "            return s60\n",
    "        if wnd == '90s':\n",
    "            return s90\n",
    "        if wnd == '120s':\n",
    "            return s120\n",
    "        if wnd == '240s':\n",
    "            return s240\n",
    "        if wnd == '600s':\n",
    "            return s600\n",
    "    \n",
    "    table = add_columns_names(table, ['Метрика', 'Размер окна, с', 'CatBoostClassifier', 'RandomForest', 'SVM-SVC', 'LogisticRegression'])\n",
    "    table = add_rows_names(table, ['Метрика', 'Accuracy'])\n",
    "    table = add_rows_names(table, ['Размер окна, с'] + \n",
    "                           [str(x).replace('s', '') for x in window_sizes] + ['Лучший результат'], col_index=WndIdx)\n",
    "    \n",
    "    best_res = {}\n",
    "    for k, v in results.items():\n",
    "        if k[0] == df_type and k[1] == window_type:\n",
    "            accuracy = results[k]['accuracy']\n",
    "            \n",
    "            if k[3] not in best_res.keys():\n",
    "                best_res[k[3]] = ('0s', 0)\n",
    "            \n",
    "            if accuracy > best_res[k[3]][1]:\n",
    "                best_res[k[3]][0] = k[2]\n",
    "                best_res[k[3]][1] = accuracy\n",
    "            \n",
    "            table.rows[get_row_idx(k[2])].cells[get_col_idx(k[3])].text = str(accuracy)\n",
    "            \n",
    "    for k, v in best_res.items():\n",
    "        table.rows[get_row_idx(v[0])].cells[get_col_idx(k)].text = str(v[1])\n",
    "            \n",
    "    word_document.add_page_break()\n",
    "    word_document.save(document_name + '.docx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
