{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, plot_roc_curve, make_scorer, f1_score, roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "from scipy import stats\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate, LeaveOneGroupOut, PredefinedSplit, GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_dataframes(path, df_type):\n",
    "    dfs_list = []\n",
    "    dfs_rows_len_list = []\n",
    "    \n",
    "    for user in os.listdir(path):\n",
    "        for file in os.listdir(os.path.join(path, user)):\n",
    "            if file.find(df_type) != -1:\n",
    "                df = pd.read_csv(os.path.join(path, user, file))\n",
    "                \n",
    "                if df_type != 'broadcasts':\n",
    "                    df = df.drop([\"timestamp\"], axis=1)\n",
    "#                 df = (df - df.min()) / (df.max() - df.min())\n",
    "                \n",
    "                df[\"user\"] = int(user.split('_')[1])\n",
    "                \n",
    "                dfs_list.append(df)\n",
    "    \n",
    "    return pd.concat(dfs_list, ignore_index=True)\n",
    "\n",
    "\n",
    "def drop_bad_rows(df, z = 3):\n",
    "    bad_rows = set()\n",
    "    for col in df.columns:\n",
    "        if col != \"user\":\n",
    "            for user in df.user.unique():\n",
    "                for x in list(df.loc[df.user == user, :][np.abs(stats.zscore(df.loc[df.user == user, col])) > z].index):\n",
    "                    bad_rows.add(x)\n",
    "\n",
    "            for x in list(df[col][np.abs(stats.zscore(df[col])) > z].index):\n",
    "                bad_rows.add(x)\n",
    "\n",
    "    df = df.drop(list(bad_rows), axis=0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_bad_cols(df, z = 3, allowed_proportion = 0.1):\n",
    "    bad_cols = set()\n",
    "    for col in df.columns:\n",
    "        if col != \"user\":\n",
    "            if df[df[col] != df[col].mean()].shape[0] < allowed_proportion * df.shape[0]:\n",
    "                bad_cols.add(col)\n",
    "\n",
    "            for user in df.user.unique():\n",
    "                if df.loc[df.user == user, :][df.loc[df.user == user, col] != df.loc[df.user == user, col].mean()].shape[0] < allowed_proportion * df.loc[df.user == user, :].shape[0]:\n",
    "                    bad_cols.add(col)\n",
    "\n",
    "                elif np.sum(np.abs(stats.zscore(df.loc[df.user == user, col])) < z) < (1 - allowed_proportion) * df.loc[df.user == user, col].shape[0]:\n",
    "                    bad_cols.add(col)\n",
    "\n",
    "    df = df.drop(bad_cols, axis=1)\n",
    "    return df, list(bad_cols)\n",
    "    \n",
    "    \n",
    "def extract_delayed_user(df, user_label):\n",
    "    df_user = df[df[\"user\"] == user_label]\n",
    "    df = df[df[\"user\"] != user_label]\n",
    "    return df_user, df\n",
    "\n",
    "\n",
    "def split_users_into_two_classes(df, valid_user_label):\n",
    "    df.loc[df[\"user\"] != valid_user_label, \"user\"] = 0\n",
    "    df.loc[df[\"user\"] == valid_user_label, \"user\"] = 1\n",
    "    return df  \n",
    "\n",
    "\n",
    "def get_cv_split(X, y, group_labels, valid_user_label):\n",
    "    predefined_split_array = np.zeros(group_labels.shape[0])\n",
    "    i = 0\n",
    "    test_array = [x for x in range(group_labels.shape[0])]\n",
    "    for test, _ in LeaveOneGroupOut().split(X, y, group_labels):\n",
    "        diff = np.setdiff1d(test_array, test)\n",
    "        if np.all(group_labels[diff[0] : diff[-1]] == valid_user_label) is np.bool_(True):\n",
    "            for sample in diff:\n",
    "                predefined_split_array[sample] = -1\n",
    "        else:\n",
    "            for sample in diff:\n",
    "                predefined_split_array[sample] = i\n",
    "            i += 1\n",
    "    return predefined_split_array\n",
    "\n",
    "\n",
    "def generate_train_dataset(df, user, ex_user, is_SVM = False):\n",
    "    df_ = df.copy()\n",
    "\n",
    "    df_for_test = []\n",
    "\n",
    "    df__ = df_[df_.labels == ex_user].copy()\n",
    "    df_for_test.append(df__)\n",
    "    df_ = df_.drop(df__.index, axis=0)\n",
    "\n",
    "    for user_ in df_.labels.unique():\n",
    "        if user_ != ex_user:\n",
    "            test_size = int((0.25 * df_[df_.labels == user_].shape[0]) - 1)\n",
    "            df__ = df_[df_.labels == user_].sample(test_size).copy()\n",
    "            df_for_test.append(df__)\n",
    "            df_ = df_.drop(df__.index, axis=0)\n",
    "\n",
    "    df_ = split_users_into_two_classes(df_.copy(), user)\n",
    "          \n",
    "    if is_SVM:    \n",
    "        df_.loc[df_.user == 0, 'user'] = -1\n",
    "\n",
    "    df_ = df_.drop(\"labels\", axis=1)\n",
    "\n",
    "    dataset = df_.to_numpy().copy()\n",
    "    np.random.shuffle(dataset)\n",
    "\n",
    "    X = dataset[:, :-1]\n",
    "    y = dataset[:, -1]\n",
    "    \n",
    "    return X, y, df_for_test\n",
    "\n",
    "\n",
    "def generate_test_dataset(df_list, user, ex_user, is_SVM = False):\n",
    "    test_df = pd.concat(df_list)\n",
    "\n",
    "    valid_user_in_test_count = test_df[test_df.labels == user].shape[0]\n",
    "    ex_user_in_test_count = test_df[test_df.labels == ex_user].shape[0]\n",
    "    others_in_test_count = [test_df[test_df.labels == x].shape[0]\n",
    "                            for x in test_df.labels.unique() if x != user and x != ex_user]\n",
    "\n",
    "    others_test_count = sum(others_in_test_count)\n",
    "    part_size = min(valid_user_in_test_count, ex_user_in_test_count)\n",
    "    if others_test_count <= min(valid_user_in_test_count, ex_user_in_test_count):\n",
    "        part_size = others_test_count    \n",
    "        \n",
    "    new_df_parts = []    \n",
    "\n",
    "    new_df_parts.append(test_df[test_df.labels == user].sample(part_size).copy())\n",
    "    new_df_parts.append(test_df[test_df.labels == ex_user].sample(part_size).copy())\n",
    "    new_df_parts.append(test_df[~test_df.labels.isin([user, ex_user])].sample(part_size).copy())\n",
    "    \n",
    "    test_df = pd.concat(new_df_parts)\n",
    "    \n",
    "    test_df.loc[test_df.labels == user, \"user\"] = 1\n",
    "    if is_SVM:\n",
    "        test_df.loc[test_df.labels != user, \"user\"] = -1\n",
    "    else:\n",
    "        test_df.loc[test_df.labels != user, \"user\"] = 0\n",
    "\n",
    "    print(\"True: \", test_df[test_df.user == 1].shape)\n",
    "    print(\"Shape: \", test_df.shape)\n",
    "    for x in test_df.labels.unique():\n",
    "        print(\"Count \", x, \": \", test_df[test_df.labels == x].shape)\n",
    "\n",
    "    test_df = test_df.drop(\"labels\", axis=1)\n",
    "\n",
    "    test_dataset = test_df.to_numpy().copy()\n",
    "    X_test = test_dataset[:, :-1].copy()\n",
    "    y_test = test_dataset[:, -1].copy()\n",
    "\n",
    "    return X_test, y_test\n",
    "\n",
    "\n",
    "def prepare_dataset(df, user, is_SVM=False):\n",
    "    df_ = split_users_into_two_classes(df.copy(), user)\n",
    "    \n",
    "    group_labels = df_.labels.to_numpy().copy()\n",
    "    df_ = df_.drop('labels', axis=1)\n",
    "    \n",
    "    if is_SVM:\n",
    "        df_.loc[df_.user == 0, 'user'] = -1\n",
    "    \n",
    "    dataset = df_.to_numpy().copy()\n",
    "    X = dataset[:, :-1]\n",
    "    y = dataset[:, -1]\n",
    "    \n",
    "    return X, y, group_labels\n",
    "\n",
    "\n",
    "def create_file_for_results(data_type):\n",
    "    res_folder = '.\\\\_results'\n",
    "    if os.path.exists(res_folder) is False:\n",
    "        os.makedirs(res_folder)\n",
    "    \n",
    "    file = os.path.join(res_folder, data_type + '_results.json')\n",
    "    if os.path.exists(file) is False:\n",
    "        with open(file, 'w') as f:\n",
    "            json.dump({'stub': None}, f)\n",
    "        \n",
    "    return file    \n",
    "\n",
    "\n",
    "def update_file_with_results(file_path, results_dict):\n",
    "    import collections.abc\n",
    "\n",
    "    def update(d, u):\n",
    "        for k, v in u.items():\n",
    "            if isinstance(v, collections.abc.Mapping):\n",
    "                d[k] = update(d.get(k, {}), v)\n",
    "            else:\n",
    "                d[k] = v\n",
    "        return d\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        res = json.load(f)\n",
    "    \n",
    "    res = update(res, results_dict)\n",
    "    \n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(res, f, sort_keys=True, indent=2)\n",
    "        \n",
    "        \n",
    "def get_dict_with_results(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        res = json.load(f)\n",
    "    return res    \n",
    "\n",
    "\n",
    "def get_dataframe(path, data_type, window_type, window_size):\n",
    "    return concat_dataframes(os.path.join(path, window_type, window_size), data_type), create_file_for_results(data_type)\n",
    "\n",
    "\n",
    "def drop_corr_columns(df, corr_coef):\n",
    "    corr_matrix = df.corr().abs()\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    corr_cols = [column for column in upper_tri.columns if any(abs(upper_tri[column]) > corr_coef) and column != \"user\"]\n",
    "    return df.drop(corr_cols, axis=1), corr_cols\n",
    "\n",
    "\n",
    "def process_train_df(df, features, corr = 0.7, z = 3, prop = 0.1): \n",
    "    df = df.drop(df.columns.difference(features), axis=1)\n",
    "    df = df.dropna(how='all')\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    if 'count_mean' in df.columns:\n",
    "        df = df[df.count_mean != 0]\n",
    "    \n",
    "    df = drop_bad_rows(df, z)\n",
    "    df, dropped_cols_1 = drop_bad_cols(df, z, prop)\n",
    "    df, dropped_cols_2 = drop_corr_columns(df, corr)\n",
    "    \n",
    "    return df, dropped_cols_1 + dropped_cols_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cross_validation(results_file, model, df, model_tag, df_type, window_type, window_size, is_SVM = False):\n",
    "    for user in df.labels.unique():\n",
    "        print(\"Valid User: \", user)\n",
    "        print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "        X, y, group_labels = prepare_dataset(df, user, is_SVM)\n",
    "\n",
    "        cv_split = PredefinedSplit(test_fold=get_cv_split(X, y, group_labels, user))\n",
    "        scoring = ('accuracy')\n",
    "\n",
    "        cv_results = cross_validate(model, X, y, scoring=scoring, cv=cv_split, n_jobs=-1)\n",
    "        accuracy = cv_results['test_score']\n",
    "        \n",
    "        results = {\n",
    "            df_type: {\n",
    "                window_type: {\n",
    "                    window_size: {\n",
    "                        model_tag: {\n",
    "                            \"cross_validation\": {\n",
    "                                \"valid_user\": {\n",
    "                                    str(user): {\n",
    "                                        \"accuracy\": accuracy.tolist()\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        update_file_with_results(results_file, results)\n",
    "\n",
    "        print(\"CV accuracy list: \", accuracy)\n",
    "        print(\"CV mean accuracy: \", np.mean(accuracy))\n",
    "        print(\"CV min accuracy: \", np.min(accuracy))\n",
    "        print(\"CV max accuracy: \", np.max(accuracy))\n",
    "\n",
    "        print(\"--------------------------------------------------------------------------------\")\n",
    "        \n",
    "        \n",
    "def model_final_validation(results_file, model, df, model_tag, df_type, window_type, window_size, is_SVM = False):\n",
    "    for user in df.labels.unique():\n",
    "        print(\"Valid User: \", user)\n",
    "        print(\"--------------------------------------------------------------------------------\")\n",
    "        for ex_user in df.labels.unique():\n",
    "            if ex_user != user:\n",
    "                X, y, df_for_test = generate_train_dataset(df, user, ex_user, is_SVM)\n",
    "\n",
    "                model.fit(X, y)\n",
    "\n",
    "                X_test, y_test = generate_test_dataset(df_for_test, user, ex_user, is_SVM)\n",
    "\n",
    "                predict = model.predict(X_test)\n",
    "                proba = model.predict_proba(X_test)\n",
    "\n",
    "                results = {\n",
    "                    df_type: {\n",
    "                        window_type: {\n",
    "                            window_size: {\n",
    "                                model_tag: {\n",
    "                                    \"final_validation\": {\n",
    "                                        \"valid_user\": {\n",
    "                                            str(user): {\n",
    "                                                \"extracted_user\": {\n",
    "                                                    str(ex_user): {\n",
    "                                                        \"test\": y_test.tolist(),\n",
    "                                                        \"predict\": predict.tolist(),\n",
    "                                                        \"proba\": proba.tolist()\n",
    "                                                    }\n",
    "                                                }\n",
    "                                            }\n",
    "                                        }\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                update_file_with_results(results_file, results)\n",
    "\n",
    "                print(\"Valid user = \", user, \", Extracted user = \", ex_user, \"accuracy = \", accuracy_score(y_test, predict))\n",
    "                print(\"--------------------------------------------------------------------------------\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning settings\n",
    "### ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '..\\\\..\\\\scripts\\\\_features_all'\n",
    "\n",
    "DATA_TYPE = \"bt\"\n",
    "\n",
    "WINDOW_TYPE = \"rolling\"\n",
    "WINDOW_SIZE = \"60s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TYPES = ['wifi', 'bt', 'location']\n",
    "WINDOW_TYPES = ['rolling', 'sampling']\n",
    "WINDOWS = ['5s', '10s', '30s', '60s', '90s', '120s', '240s', '600s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_params = {\n",
    "    'iterations': 100,\n",
    "    'depth': 6,\n",
    "    'loss_function': 'Logloss',\n",
    "    'l2_leaf_reg': 1,\n",
    "    'leaf_estimation_iterations': 5,\n",
    "    'logging_level': 'Silent'\n",
    "}\n",
    "\n",
    "randomforest_params = {\n",
    "    'n_estimators': 100,\n",
    "    'criterion': 'gini',\n",
    "    'max_depth': None,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'max_features': 'auto',\n",
    "    'n_jobs': -1,\n",
    "    'class_weight': 'balanced',\n",
    "}\n",
    "\n",
    "svc_params = {\n",
    "    'C': 1,\n",
    "    'kernel': 'rbf',\n",
    "    'degree': 1,\n",
    "    'gamma': 5,\n",
    "    'probability': True\n",
    "}\n",
    "\n",
    "logreg_params = {\n",
    "    'penalty': 'l2',\n",
    "    'C': 0.01,\n",
    "    'solver': 'newton-cg',\n",
    "    'max_iter': 1000,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "MODELS = [\n",
    "    (CatBoostClassifier(**catboost_params), \"CatBoost\"),\n",
    "    (RandomForestClassifier(**randomforest_params), \"RandomForest\"),\n",
    "    (SVC(**svc_params), \"SVC\"),\n",
    "    (LogisticRegression(**logreg_params), \"LogReg\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_type in DATA_TYPES:\n",
    "    for wnd_type in WINDOW_TYPES:\n",
    "        for wnd in WINDOWS:\n",
    "            df, RESULTS_FILE = get_dataframe(DATA_PATH, data_type, wnd_type, wnd)\n",
    "            \n",
    "            features = df.columns.to_list()\n",
    "            df, _ = process_train_df(df, features)\n",
    "            df['labels'] = df['user']\n",
    "            \n",
    "            for model, tag in MODELS:\n",
    "                print(data_type, wnd_type, wnd, tag)\n",
    "                model_cross_validation(RESULTS_FILE, model, df, tag, data_type, wnd_type, wnd, is_SVM=tag=='SVC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_type in DATA_TYPES:\n",
    "    for wnd_type in WINDOW_TYPES:\n",
    "        for wnd in WINDOWS:\n",
    "            df, RESULTS_FILE = get_dataframe(DATA_PATH, data_type, wnd_type, wnd)\n",
    "            \n",
    "            features = df.columns.to_list()\n",
    "            df, _ = process_train_df(df, features)\n",
    "            df['labels'] = df['user']\n",
    "                \n",
    "            for model, tag in MODELS:\n",
    "                print(data_type, wnd_type, wnd, tag)\n",
    "                model_final_validation(RESULTS_FILE, model, df, tag, data_type, wnd_type, wnd, is_SVM=tag=='SVC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
