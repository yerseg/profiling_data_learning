{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime as dt\n",
    "from scipy.spatial import distance\n",
    "import scipy.stats as stats\n",
    "from geopy.distance import distance as geodist\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bt_file_process(filepath):\n",
    "    \n",
    "    new_files_dir = \"_generated\"\n",
    "    \n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    if os.path.exists(os.path.join(os.getcwd(), new_files_dir)) is False:\n",
    "        os.mkdir(os.path.join(os.getcwd(), new_files_dir))\n",
    "    \n",
    "    new_file_path = os.path.join(os.getcwd(), new_files_dir, os.path.basename(filepath))\n",
    "    new_le_file_path = os.path.join(os.getcwd(), new_files_dir, \"_\".join([\"le\", os.path.basename(filepath)]))\n",
    "    \n",
    "    with open(new_file_path, 'w+', encoding='utf-8') as f:\n",
    "        for line, i in zip(lines, range(len(lines))):\n",
    "            if line.find(';LE;') == -1:\n",
    "                f.write(line)\n",
    "    \n",
    "    with open(new_le_file_path, 'w+', encoding='utf-8') as f:\n",
    "        for line, i in zip(lines, range(len(lines))):\n",
    "            if line.find(';LE;') != -1:\n",
    "                f.write(line)\n",
    "                \n",
    "    return {'BASE': new_file_path, 'LE': new_le_file_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bt_make_dataframes(file_path, le_file_path, sampling_freq, rolling = False, sampling = True):\n",
    "    df = pd.read_csv(file_path, index_col = False, header = None, low_memory = False, \\\n",
    "                         names = ['timestamp', 'ssid', 'bssid', 'major_class', 'class', \\\n",
    "                                      'bond_state', 'type'])\n",
    "    \n",
    "    new_files_dir = \"_datasets\"\n",
    "    if os.path.exists(os.path.join(os.getcwd(), new_files_dir)) is False:\n",
    "        os.mkdir(os.path.join(os.getcwd(), new_files_dir))\n",
    "        \n",
    "    new_files_dir += \"\\\\\" + sampling_freq\n",
    "    if os.path.exists(os.path.join(os.getcwd(), new_files_dir)) is False:\n",
    "        os.mkdir(os.path.join(os.getcwd(), new_files_dir))\n",
    "\n",
    "    df['timestamp'] = df['timestamp'].apply(lambda x: dt.strptime(x, '%d.%m.%Y_%H:%M:%S.%f'))\n",
    "\n",
    "    one_hot_maj_class = pd.get_dummies(df['major_class'], prefix='one_hot_c')\n",
    "    df = df.join(one_hot_maj_class)\n",
    "\n",
    "    one_hot_bond = pd.get_dummies(df['bond_state'], prefix='one_hot_b')\n",
    "    df = df.join(one_hot_bond)\n",
    "\n",
    "    one_hot_type = pd.get_dummies(df['type'], prefix='one_hot_t')\n",
    "    df = df.join(one_hot_type)\n",
    "\n",
    "    df.index = pd.DatetimeIndex(df.timestamp)\n",
    "    df = df.sort_index()\n",
    "\n",
    "    df = df.drop(['timestamp', 'ssid', 'class', 'major_class', 'bond_state', 'type'], axis = 1)\n",
    "\n",
    "    bssid_map = { bssid.replace(' ', ''): idx for bssid, idx in zip(df.bssid.unique(), range(len(df.bssid.unique()))) }\n",
    "    df.bssid = df.bssid.apply(lambda x: str(x).replace(' ', ''))\n",
    "    df['count'] = 1\n",
    "\n",
    "    def agg_string_join(col):\n",
    "        col = col.apply(lambda x: str(x))\n",
    "        return col.str.cat(sep = ',').replace(' ', '')\n",
    "\n",
    "    def agg_bssid_col(col):\n",
    "        array_len = len(bssid_map)\n",
    "        array = np.zeros(array_len, dtype = 'int8')\n",
    "        def fill_array(bssid):\n",
    "            array[bssid_map[bssid.replace(' ', '')]] = 1\n",
    "            return\n",
    "\n",
    "        col.apply(lambda x: fill_array(x))\n",
    "        return np.array2string(array, separator = ',').replace(' ', '')[1:-1]\n",
    "    \n",
    "    one_hot_columns_count = 0\n",
    "    for col in df.columns:\n",
    "        if col.find('one_hot') != -1:\n",
    "            one_hot_columns_count += 1\n",
    "\n",
    "    cat_columns = df.columns[1:1 + one_hot_columns_count]\n",
    "    cat_columns_map = { col: 'mean' for col in cat_columns }\n",
    "\n",
    "    all_func_dicts_quantum = { 'bssid' : agg_bssid_col, 'count' : 'sum' }\n",
    "    all_func_dicts_quantum.update(cat_columns_map)\n",
    "\n",
    "    df_quantum = df.groupby(pd.Grouper(freq = '5s'), as_index=True).agg(all_func_dicts_quantum)\n",
    "\n",
    "    df_quantum = df_quantum.reset_index()\n",
    "    df_quantum.index = pd.DatetimeIndex(df_quantum.timestamp)\n",
    "\n",
    "    df_quantum = df_quantum.dropna()\n",
    "\n",
    "    df_le = pd.read_csv(le_file_path, index_col = False, header = None, low_memory = False, \\\n",
    "                        names = ['timestamp', 'stamp', '1', '2', 'level', '3', 'connectable', '8'])\n",
    "\n",
    "    df_le = df_le.drop(df_le.columns.difference(['connectable','timestamp', 'level']), axis = 1)\n",
    "    df_le['timestamp'] = df_le['timestamp'].apply(lambda x: dt.strptime(x, '%d.%m.%Y_%H:%M:%S.%f'))\n",
    "    df_le.index = pd.DatetimeIndex(df_le.timestamp)\n",
    "    df_le = df_le.sort_index()\n",
    "\n",
    "    df_le['connectable'] = df_le['connectable'].apply(lambda x: 1 if str(x).lower() == 'true' else 0)\n",
    "\n",
    "    df_le = df_le.groupby(pd.Grouper(freq = '5s'), as_index=True).agg({'level':'mean', 'connectable':'mean'})\n",
    "\n",
    "    df_le = df_le.dropna()\n",
    "\n",
    "    def get_le_conn_status_from_row(row):\n",
    "        conn = df_le.iloc[df_le.index.get_loc(row.name, method = 'nearest')]['connectable']\n",
    "        time = df_le.iloc[df_le.index.get_loc(row.name, method = 'nearest')].name\n",
    "        return conn if abs((time - row.name).total_seconds()) < 10 else 0\n",
    "\n",
    "    def get_le_level_from_row(row):\n",
    "        level = df_le.iloc[df_le.index.get_loc(row.name, method = 'nearest')]['level']\n",
    "        time = df_le.iloc[df_le.index.get_loc(row.name, method = 'nearest')].name\n",
    "        return level if abs((time - row.name).total_seconds()) < 10 else 0\n",
    "\n",
    "\n",
    "    df_quantum['le_connectable'] = df_quantum.apply(lambda row: get_le_conn_status_from_row(row), axis = 1)\n",
    "    df_quantum['le_level'] = df_quantum.apply(lambda row: get_le_level_from_row(row), axis = 1)\n",
    "\n",
    "    def string2array(string):\n",
    "        try:\n",
    "            array = np.fromstring(string, sep=',')\n",
    "            return array\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    def to_ones_array(array):\n",
    "        try:\n",
    "            array[array != 0] = 1\n",
    "            return array\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    def get_len(obj):\n",
    "        try:\n",
    "            length = len(obj)\n",
    "            return length\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    def get_occured_nets_count(row, prev_col, curr_col):\n",
    "        prev = to_ones_array(string2array(row[prev_col]))\n",
    "        curr = to_ones_array(string2array(row[curr_col]))\n",
    "        intersection = np.logical_and(curr, prev)\n",
    "        diff = np.logical_and(curr, np.logical_not(intersection))\n",
    "\n",
    "        if (np.count_nonzero(np.logical_or(prev, curr)) == 0):\n",
    "            return 0\n",
    "\n",
    "        return np.count_nonzero(diff) / np.count_nonzero(np.logical_or(prev, curr))\n",
    "\n",
    "    def get_disappeared_nets_count(row, prev_col, curr_col):\n",
    "        prev = to_ones_array(string2array(row[prev_col]))\n",
    "        curr = to_ones_array(string2array(row[curr_col]))\n",
    "        intersection = np.logical_and(curr, prev)\n",
    "        diff = np.logical_and(prev, np.logical_not(intersection))\n",
    "\n",
    "        if (np.count_nonzero(np.logical_or(prev, curr)) == 0):\n",
    "            return 0\n",
    "\n",
    "        return np.count_nonzero(diff) / np.count_nonzero(np.logical_or(prev, curr))\n",
    "\n",
    "    def get_jaccard_index(row, prev_col, curr_col):\n",
    "        prev = to_ones_array(string2array(row[prev_col]))\n",
    "        curr = to_ones_array(string2array(row[curr_col]))\n",
    "        return distance.jaccard(prev, curr)\n",
    "\n",
    "    def get_occur_speed(row, prev_col, curr_col):\n",
    "        prev = to_ones_array(string2array(row[prev_col]))\n",
    "        curr = to_ones_array(string2array(row[curr_col]))\n",
    "        return np.linalg.norm(prev - curr) / np.sqrt(get_len(prev))\n",
    "\n",
    "    def get_level_speed(row, prev_col, curr_col):\n",
    "        prev = string2array(row[prev_col])\n",
    "        curr = string2array(row[curr_col])\n",
    "        return np.linalg.norm(prev - curr) / np.sqrt(get_len(prev))\n",
    "\n",
    "    def calc_single_cols_in_window(df, col, new_col, window, func):\n",
    "        def func_wrapper(func, row, prev_col, curr_col):\n",
    "            delta = row.timestamp - row.prev_timestamp\n",
    "            if pd.isnull(delta):\n",
    "                delta = 0\n",
    "            else:\n",
    "                delta = abs(delta.total_seconds())\n",
    "            if delta > 10 * 60:\n",
    "                return np.nan\n",
    "            else:\n",
    "                return func(row, prev_col_name, col)\n",
    "\n",
    "        new_cols = []\n",
    "\n",
    "        for i in range(window):\n",
    "            prev_col_name = \"_\".join(['prev', col, str(i + 1)])\n",
    "            new_col_name = \"_\".join([new_col, str(i + 1)])\n",
    "\n",
    "            df.loc[:, 'prev_timestamp'] = df.timestamp.shift(i + 1)\n",
    "            df.loc[:, prev_col_name] = df[col].shift(i + 1)\n",
    "            df.loc[:, new_col_name] = df.apply(lambda row: func_wrapper(func, row, prev_col_name, col), axis = 1)\n",
    "            df = df.drop(prev_col_name, axis = 1)\n",
    "            df = df.drop('prev_timestamp', axis = 1)\n",
    "            new_cols.append(new_col_name)\n",
    "\n",
    "        df.loc[:, \"_\".join([new_col, 'mean'])] = df[new_cols].mean(axis = 1)\n",
    "        df.loc[:, \"_\".join([new_col, 'median'])] = df[new_cols].median(axis = 1)\n",
    "        df.loc[:, \"_\".join([new_col, 'var'])] = df[new_cols].var(axis = 1)\n",
    "\n",
    "        return df\n",
    "\n",
    "    WINDOW_SIZE = 3\n",
    "\n",
    "    occur_and_level_columns_map = [\n",
    "        (\"bssid\", \"occured_devices_count\", WINDOW_SIZE, get_occured_nets_count),\n",
    "        (\"bssid\", \"disappeared_devices_count\", WINDOW_SIZE, get_disappeared_nets_count),\n",
    "        (\"bssid\", \"jaccard_index\", WINDOW_SIZE, get_jaccard_index), \n",
    "        (\"bssid\", \"occur_speed\", WINDOW_SIZE, get_occur_speed)\n",
    "    ]\n",
    "\n",
    "    for (col, new_col, window, func) in occur_and_level_columns_map:\n",
    "        df_quantum = calc_single_cols_in_window(df_quantum, col, new_col, window, func)\n",
    "\n",
    "    def get_conn_level_speed(row, prev_col, curr_col):\n",
    "        return row[curr_col] - row[prev_col]\n",
    "\n",
    "    WINDOW_SIZE = 3\n",
    "\n",
    "    single_columns_map = [\n",
    "        (\"count\", \"count_speed\", WINDOW_SIZE, get_conn_level_speed)\n",
    "    ]\n",
    "\n",
    "    for (col, new_col, window, func) in single_columns_map:\n",
    "        df_quantum = calc_single_cols_in_window(df_quantum, col, new_col, window, func)\n",
    "\n",
    "    def get_acceleration(row, prev_col, curr_col):\n",
    "        return abs(row[curr_col] - row[prev_col])\n",
    "\n",
    "    WINDOW_SIZE = 3\n",
    "\n",
    "    multi_speed_cols = [\"occured_devices_count_mean\", \"jaccard_index_mean\", \"occur_speed_mean\", \"disappeared_devices_count_mean\", \n",
    "                        \"count_speed_mean\"]\n",
    "\n",
    "    for col in multi_speed_cols:\n",
    "        df_quantum = calc_single_cols_in_window(df_quantum, col, \"_\".join([\"acceleration\", col]), window, func)\n",
    "\n",
    "    def agg_str(col):\n",
    "        all_freq = col.str.cat(sep=',')\n",
    "        return string2array(all_freq)\n",
    "\n",
    "    def str_mean(col):\n",
    "        return np.mean(agg_str(col))\n",
    "\n",
    "    def str_var(col):\n",
    "        return np.var(agg_str(col))\n",
    "\n",
    "    def str_median(col):\n",
    "        return np.median(agg_str(col))\n",
    "\n",
    "    def str_skew(col):\n",
    "        return stats.skew(agg_str(col))\n",
    "\n",
    "    def str_kurt(col):\n",
    "        return stats.kurtosis(agg_str(col))\n",
    "\n",
    "    def mean(col):\n",
    "        return np.mean(col)\n",
    "\n",
    "    def var(col):\n",
    "        return np.var(col)\n",
    "\n",
    "    def median(col):\n",
    "        return np.median(col)\n",
    "\n",
    "    def skew(col):\n",
    "        return stats.skew(col)\n",
    "\n",
    "    def kurt(col):\n",
    "        return stats.kurtosis(col)\n",
    "\n",
    "    df_quantum = df_quantum.drop(['bssid', 'timestamp'], axis = 1)\n",
    "\n",
    "    common_cols = df_quantum.columns[:one_hot_columns_count + 3]\n",
    "    speed_acc_cols = df_quantum.columns[one_hot_columns_count + 3:]\n",
    "\n",
    "    common_funcs_list = [mean, var, median, skew, kurt]\n",
    "    special_funcs_list = [mean, pd.DataFrame.mad, skew]\n",
    "\n",
    "    common_cols_map = { col : common_funcs_list for col in common_cols }\n",
    "    speed_acc_cols_map = { col : special_funcs_list for col in speed_acc_cols }\n",
    "\n",
    "    agg_dict = common_cols_map\n",
    "    agg_dict.update(speed_acc_cols_map)\n",
    "\n",
    "    df_quantum[speed_acc_cols] = df_quantum[speed_acc_cols].apply(pd.to_numeric)\n",
    "\n",
    "    df_sampling = df_quantum.groupby(pd.Grouper(freq = sampling_freq)).agg(agg_dict)\n",
    "    df_sampling.columns = [\"_\".join([str(high_level_name), str(low_level_name)]) \\\n",
    "                               for (high_level_name, low_level_name) in df_sampling.columns.values]\n",
    "    \n",
    "    df_sampling = df_sampling.dropna()\n",
    "    df_sampling = df_sampling.fillna(0)\n",
    "    \n",
    "    index = os.path.basename(file_path).split('_')[-1][0]\n",
    "    \n",
    "    df_sampling.to_csv(new_files_dir + \"\\\\bt_sampling_dataset_\" + index + \".csv\")\n",
    "    \n",
    "    df_rolling = df_quantum.rolling(sampling_freq, min_periods = 1, center = False).agg(agg_dict)\n",
    "    \n",
    "    df_rolling.columns = [\"_\".join([str(high_level_name), str(low_level_name)]) \\\n",
    "                              for (high_level_name, low_level_name) in df_rolling.columns.values]\n",
    "\n",
    "    df_rolling = df_rolling.dropna()\n",
    "    df_rolling = df_rolling.fillna(0)\n",
    "\n",
    "    df_rolling.to_csv(new_files_dir + \"\\\\bt_rolling_dataset_\" + index + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bt_pipeline(files, sampling_freq):\n",
    "    logs = []\n",
    "    for file in files:\n",
    "        logs.append(bt_file_process(file))\n",
    "    for t in logs:\n",
    "        print(t['BASE'], sampling_freq)\n",
    "        bt_make_dataframes(t['BASE'], t['LE'], sampling_freq, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_FREQs = ['5s', '10s', '30s', '60s', '90s', '120s', '240s', '600s']\n",
    "data_list = [\n",
    "    \".\\\\raw_data\\\\bt_1.data\",\n",
    "    \".\\\\raw_data\\\\bt_2.data\",\n",
    "    \".\\\\raw_data\\\\bt_3.data\",\n",
    "    \".\\\\raw_data\\\\bt_4.data\",\n",
    "    \".\\\\raw_data\\\\bt_5.data\",\n",
    "    \".\\\\raw_data\\\\bt_6.data\",\n",
    "    \".\\\\raw_data\\\\bt_7.data\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Dev\\nir\\_generated\\bt_1.data 5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "D:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "D:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:3405: RuntimeWarning: Invalid value encountered in median\n",
      "  r = func(a, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Dev\\nir\\_generated\\bt_2.data 5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "D:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "D:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:3405: RuntimeWarning: Invalid value encountered in median\n",
      "  r = func(a, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Dev\\nir\\_generated\\bt_3.data 5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "D:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Dev\\nir\\_generated\\bt_4.data 5s\n",
      "D:\\Dev\\nir\\_generated\\bt_5.data 5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "D:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "D:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:3405: RuntimeWarning: Invalid value encountered in median\n",
      "  r = func(a, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "for freq in SAMPLING_FREQs:\n",
    "    bt_pipeline(data_list, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# bt_pipeline([\".\\\\user_1\\\\wifi\\\\wifi_0.data\"], SAMPLING_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bt_file_process(\".\\\\bt_7.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}