{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime as dt\n",
    "from scipy.spatial import distance\n",
    "import scipy.stats as stats\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wifi_file_process(filepath):\n",
    "    \n",
    "    new_files_dir = \"_generated\"\n",
    "    \n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    if os.path.exists(os.path.join(os.getcwd(), new_files_dir)) is False:\n",
    "        os.mkdir(os.path.join(os.getcwd(), new_files_dir))\n",
    "    \n",
    "    new_file_path = os.path.join(os.getcwd(), new_files_dir, os.path.basename(filepath))\n",
    "    new_conn_file_path = os.path.join(os.getcwd(), new_files_dir, \"_\".join([\"conn\", os.path.basename(filepath)]))\n",
    "    \n",
    "    with open(new_file_path, 'w+', encoding='utf-8') as f:\n",
    "        for line, i in zip(lines, range(len(lines))):\n",
    "            if line.find(';CONN;') == -1:\n",
    "                f.write(line)\n",
    "            \n",
    "    with open(new_conn_file_path, 'w+', encoding='utf-8') as f:\n",
    "        for line, i in zip(lines, range(len(lines))):\n",
    "            if line.find(';CONN;') != -1:\n",
    "                f.write(line)\n",
    "                \n",
    "    return {'BASE': new_file_path, 'CONN': new_conn_file_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wifi_make_dfs(file_path, conn_file_path, sampling_freq, rolling = False, sampling = True):\n",
    "    df = pd.read_csv(file_path, index_col = False, header = None, low_memory = False, \\\n",
    "                     names = ['timestamp', 'uuid', 'bssid', 'ssid', 'capabilities', \\\n",
    "                              'freq0', 'freq1', 'chwidth', 'freq', 'level', 'operator', \\\n",
    "                              'time', 'venue', '802', 'passpoint'])\n",
    "    \n",
    "    new_files_dir = \"_datasets\"\n",
    "    if os.path.exists(os.path.join(os.getcwd(), new_files_dir)) is False:\n",
    "        os.mkdir(os.path.join(os.getcwd(), new_files_dir))\n",
    "        \n",
    "    new_files_dir += \"\\\\\" + sampling_freq\n",
    "    if os.path.exists(os.path.join(os.getcwd(), new_files_dir)) is False:\n",
    "        os.mkdir(os.path.join(os.getcwd(), new_files_dir))\n",
    "\n",
    "    df['timestamp'] = df['timestamp'].apply(lambda x: dt.strptime(x, '%d.%m.%Y_%H:%M:%S.%f'))\n",
    "\n",
    "    df.index = pd.DatetimeIndex(df.timestamp)\n",
    "    df = df.sort_index()\n",
    "\n",
    "    df = df.drop(['timestamp', 'chwidth', 'operator', 'venue', '802', 'time', 'ssid', 'capabilities', \\\n",
    "                  'passpoint', 'freq0', 'freq1'], axis = 1)\n",
    "\n",
    "    bssid_map = { bssid.replace(' ', ''): idx for bssid, idx in zip(df.bssid.unique(), range(len(df.bssid.unique()))) }\n",
    "\n",
    "    df.bssid = df.bssid.apply(lambda x: str(x).replace(' ', ''))\n",
    "    df.level = df.level.apply(lambda x: str(x).replace(' ', ''))\n",
    "    df.freq = df.freq.apply(lambda x: str(x).replace(' ', ''))\n",
    "\n",
    "    df['bssid_level'] = df[['bssid', 'level']].agg(','.join, axis=1)\n",
    "    df['count'] = 1\n",
    "\n",
    "    def agg_string_join(col):\n",
    "        col = col.apply(lambda x: str(x))\n",
    "        return col.str.cat(sep = ',').replace(' ', '')\n",
    "\n",
    "    def agg_bssid_col(col):\n",
    "        array_len = len(bssid_map)\n",
    "        array = np.zeros(array_len, dtype = 'float')\n",
    "        def fill_array(x):\n",
    "            tmp = x.split(',')\n",
    "            bssid = tmp[0]\n",
    "            level = float(tmp[1])\n",
    "            array[bssid_map[bssid.replace(' ', '')]] = level\n",
    "            return\n",
    "\n",
    "        col.apply(lambda x: fill_array(x))\n",
    "        return np.array2string(array, separator = ',').replace(' ', '')[1:-1]\n",
    "\n",
    "    all_func_dicts_quantum = { 'freq': agg_string_join, 'level': agg_string_join, 'bssid_level' : agg_bssid_col, 'count' : 'sum' }\n",
    "\n",
    "    df_quantum = df.groupby(['timestamp', 'uuid'], as_index=True).agg(all_func_dicts_quantum)\n",
    "\n",
    "    df_quantum = df_quantum.reset_index()\n",
    "    df_quantum.index = pd.DatetimeIndex(df_quantum.timestamp)\n",
    "\n",
    "    df_quantum = df_quantum[df_quantum['count'] != 0]\n",
    "\n",
    "    df_conn = pd.read_csv(conn_file_path, index_col = False, header = None, low_memory = False, \\\n",
    "                          names = ['timestamp', 'uuid', 'stamp', 'bssid', '1', '2', '3', '4', '5', '6', 'level', '8'])\n",
    "\n",
    "    df_conn = df_conn.drop(df_conn.columns.difference(['bssid','timestamp', 'level']), axis = 1)\n",
    "    df_conn['timestamp'] = df_conn['timestamp'].apply(lambda x: dt.strptime(x, '%d.%m.%Y_%H:%M:%S.%f'))\n",
    "    df_conn.index = pd.DatetimeIndex(df_conn.timestamp)\n",
    "    df_conn = df_conn.sort_index()\n",
    "\n",
    "    def get_level_from_row(row):\n",
    "        bssid = df_conn.iloc[df_conn.index.get_loc(row.name, method = 'nearest')]['bssid']\n",
    "        if str(bssid) == 'nan' or str(bssid) == 'null' or str(bssid) == '':\n",
    "            return 0\n",
    "\n",
    "        level = df_conn.iloc[df_conn.index.get_loc(row.name, method = 'nearest')]['level']\n",
    "        time = df_conn.iloc[df_conn.index.get_loc(row.name, method = 'nearest')]['timestamp']\n",
    "        return level if abs((time - row.name).total_seconds()) <= 10 else 0\n",
    "\n",
    "    df_quantum['conn_level'] = df_quantum.apply(lambda row: get_level_from_row(row), axis = 1)\n",
    "\n",
    "    def string2array(string):\n",
    "        try:\n",
    "            array = np.fromstring(string, sep=',')\n",
    "            return array\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    def to_ones_array(array):\n",
    "        try:\n",
    "            array[array != 0] = 1\n",
    "            return array\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    def get_len(obj):\n",
    "        try:\n",
    "            length = len(obj)\n",
    "            return length\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    def get_occured_nets_count(row, prev_col, curr_col):\n",
    "        prev = to_ones_array(string2array(row[prev_col]))\n",
    "        curr = to_ones_array(string2array(row[curr_col]))\n",
    "        intersection = np.logical_and(curr, prev)\n",
    "        diff = np.logical_and(curr, np.logical_not(intersection))\n",
    "\n",
    "        if (np.count_nonzero(np.logical_or(prev, curr)) == 0):\n",
    "            return 0\n",
    "\n",
    "        return np.count_nonzero(diff) / np.count_nonzero(np.logical_or(prev, curr))\n",
    "\n",
    "    def get_disappeared_nets_count(row, prev_col, curr_col):\n",
    "        prev = to_ones_array(string2array(row[prev_col]))\n",
    "        curr = to_ones_array(string2array(row[curr_col]))\n",
    "        intersection = np.logical_and(curr, prev)\n",
    "        diff = np.logical_and(prev, np.logical_not(intersection))\n",
    "\n",
    "        if (np.count_nonzero(np.logical_or(prev, curr)) == 0):\n",
    "            return 0\n",
    "\n",
    "        return np.count_nonzero(diff) / np.count_nonzero(np.logical_or(prev, curr))\n",
    "\n",
    "    def get_jaccard_index(row, prev_col, curr_col):\n",
    "        prev = to_ones_array(string2array(row[prev_col]))\n",
    "        curr = to_ones_array(string2array(row[curr_col]))\n",
    "        return distance.jaccard(prev, curr)\n",
    "\n",
    "    def get_occur_speed(row, prev_col, curr_col):\n",
    "        prev = to_ones_array(string2array(row[prev_col]))\n",
    "        curr = to_ones_array(string2array(row[curr_col]))\n",
    "        return np.linalg.norm(prev - curr) / np.sqrt(get_len(prev))\n",
    "\n",
    "    def get_level_speed(row, prev_col, curr_col):\n",
    "        prev = string2array(row[prev_col])\n",
    "        curr = string2array(row[curr_col])\n",
    "        return np.linalg.norm(prev - curr) / np.sqrt(get_len(prev))\n",
    "\n",
    "    def calc_single_cols_in_window(df, col, new_col, window, func):\n",
    "        def func_wrapper(func, row, prev_col, curr_col):\n",
    "            delta = row.timestamp - row.prev_timestamp\n",
    "            if pd.isnull(delta):\n",
    "                delta = 0\n",
    "            else:\n",
    "                delta = abs(delta.total_seconds())\n",
    "            if delta > 10 * 60:\n",
    "                return np.nan\n",
    "            else:\n",
    "                return func(row, prev_col_name, col)\n",
    "\n",
    "        new_cols = []\n",
    "\n",
    "        for i in range(window):\n",
    "            prev_col_name = \"_\".join(['prev', col, str(i + 1)])\n",
    "            new_col_name = \"_\".join([new_col, str(i + 1)])\n",
    "\n",
    "            df['prev_timestamp'] = df.timestamp.shift(i + 1)\n",
    "            df[prev_col_name] = df[col].shift(i + 1)\n",
    "            df[new_col_name] = df.apply(lambda row: func_wrapper(func, row, prev_col_name, col), axis = 1)\n",
    "            df = df.drop(prev_col_name, axis = 1)\n",
    "            df = df.drop('prev_timestamp', axis = 1)\n",
    "            new_cols.append(new_col_name)\n",
    "\n",
    "        df[\"_\".join([new_col, 'mean'])] = df[new_cols].mean(axis = 1)\n",
    "        df[\"_\".join([new_col, 'median'])] = df[new_cols].median(axis = 1)\n",
    "        df[\"_\".join([new_col, 'var'])] = df[new_cols].var(axis = 1)\n",
    "\n",
    "        return df\n",
    "\n",
    "    WINDOW_SIZE = 3\n",
    "\n",
    "    occur_and_level_columns_map = [\n",
    "        (\"bssid_level\", \"occured_nets_count\", WINDOW_SIZE, get_occured_nets_count),\n",
    "        (\"bssid_level\", \"disappeared_nets_count\", WINDOW_SIZE, get_disappeared_nets_count),\n",
    "        (\"bssid_level\", \"jaccard_index\", WINDOW_SIZE, get_jaccard_index), \n",
    "        (\"bssid_level\", \"occur_speed\", WINDOW_SIZE, get_occur_speed),\n",
    "        (\"bssid_level\", \"level_speed\", WINDOW_SIZE, get_level_speed)\n",
    "    ]\n",
    "\n",
    "    for (col, new_col, window, func) in occur_and_level_columns_map:\n",
    "        df_quantum = calc_single_cols_in_window(df_quantum, col, new_col, window, func)\n",
    "\n",
    "    def get_conn_level_speed(row, prev_col, curr_col):\n",
    "        return row[curr_col] - row[prev_col]\n",
    "\n",
    "    WINDOW_SIZE = 3\n",
    "\n",
    "    single_columns_map = [\n",
    "        (\"conn_level\", \"conn_level_speed\", WINDOW_SIZE, get_conn_level_speed),\n",
    "        (\"count\", \"count_speed\", WINDOW_SIZE, get_conn_level_speed)\n",
    "    ]\n",
    "\n",
    "    for (col, new_col, window, func) in single_columns_map:\n",
    "        df_quantum = calc_single_cols_in_window(df_quantum, col, new_col, window, func)\n",
    "\n",
    "    def get_acceleration(row, prev_col, curr_col):\n",
    "        return abs(row[curr_col] - row[prev_col])\n",
    "\n",
    "    WINDOW_SIZE = 3\n",
    "\n",
    "    multi_speed_cols = [\"occured_nets_count_mean\", \"jaccard_index_mean\", \"occur_speed_mean\", \"disappeared_nets_count_mean\", \n",
    "                        \"conn_level_speed_mean\", \"count_speed_mean\"]\n",
    "\n",
    "    for col in multi_speed_cols:\n",
    "        df_quantum = calc_single_cols_in_window(df_quantum, col, \"_\".join([\"acceleration\", col]), window, func)\n",
    "\n",
    "    def agg_str(col):\n",
    "        return string2array(col)\n",
    "\n",
    "    def str_mean(col):\n",
    "        array = agg_str(col)\n",
    "        if str(array) == 'nan':\n",
    "            return 0 \n",
    "        return np.mean(array)\n",
    "\n",
    "    def mean(col):\n",
    "        return np.mean(col)\n",
    "\n",
    "    def var(col):\n",
    "        return np.var(col)\n",
    "\n",
    "    def median(col):\n",
    "        return np.median(col)\n",
    "\n",
    "    def skew(col):\n",
    "        return stats.skew(col)\n",
    "\n",
    "    def kurt(col):\n",
    "        return stats.kurtosis(col)\n",
    "\n",
    "    df_quantum['freq'] = df_quantum.apply(lambda row: str_mean(row['freq']), axis = 1)\n",
    "    df_quantum['level'] = df_quantum.apply(lambda row: str_mean(row['level']), axis = 1)\n",
    "\n",
    "    df_quantum = df_quantum.drop(['bssid_level', 'timestamp', 'uuid'], axis = 1)\n",
    "\n",
    "    common_cols = df_quantum.columns[1:5]\n",
    "    speed_acc_cols = df_quantum.columns[5:]\n",
    "\n",
    "    common_funcs_list = [mean, var, median, skew, kurt]\n",
    "    special_funcs_list = [mean, pd.DataFrame.mad, skew]\n",
    "\n",
    "    common_cols_map = { col : common_funcs_list for col in common_cols }\n",
    "    speed_acc_cols_map = { col : special_funcs_list for col in speed_acc_cols }\n",
    "\n",
    "    agg_dict = common_cols_map\n",
    "    agg_dict.update(speed_acc_cols_map)\n",
    "\n",
    "    df_quantum[speed_acc_cols] = df_quantum[speed_acc_cols].apply(pd.to_numeric)\n",
    "\n",
    "    df_sampling = df_quantum.groupby(pd.Grouper(freq = sampling_freq)).agg(agg_dict)\n",
    "    \n",
    "    df_sampling.columns = [\"_\".join([str(high_level_name), str(low_level_name)]) \\\n",
    "                               for (high_level_name, low_level_name) in df_sampling.columns.values]\n",
    "    \n",
    "    df_sampling = df_sampling.dropna()\n",
    "    df_sampling = df_sampling.fillna(0)\n",
    "    \n",
    "    index = os.path.basename(file_path).split('_')[-1][0]\n",
    "    \n",
    "    df_sampling.to_csv(new_files_dir + \"\\\\wifi_sampling_dataset_\" + index + \".csv\")\n",
    "\n",
    "    df_rolling = df_quantum.rolling(sampling_freq, min_periods = 1, center = False).agg(agg_dict)\n",
    "\n",
    "    df_rolling.columns = [\"_\".join([str(high_level_name), str(low_level_name)]) \\\n",
    "                              for (high_level_name, low_level_name) in df_rolling.columns.values]\n",
    "\n",
    "    df_rolling = df_rolling.dropna()\n",
    "    df_rolling = df_rolling.fillna(0)\n",
    "\n",
    "    df_rolling.to_csv(new_files_dir + \"\\\\wifi_rolling_dataset_\" + index + \".csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wifi_pipeline(files, sampling_freq):\n",
    "    logs = []\n",
    "    for file in files:\n",
    "        logs.append(wifi_file_process(file))\n",
    "    for t in logs:\n",
    "        print(t['BASE'], sampling_freq)\n",
    "        wifi_make_dfs(t['BASE'], t['CONN'], sampling_freq, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_FREQs = ['5s', '10s', '30s', '60s', '90s', '120s', '240s', '600s']\n",
    "data_list = [\n",
    "    \".\\\\raw_data\\\\wifi_1.data\",\n",
    "    \".\\\\raw_data\\\\wifi_2.data\",\n",
    "    \".\\\\raw_data\\\\wifi_3.data\",\n",
    "    \".\\\\raw_data\\\\wifi_4.data\",\n",
    "    \".\\\\raw_data\\\\wifi_5.data\",\n",
    "    \".\\\\raw_data\\\\wifi_6.data\",\n",
    "    \".\\\\raw_data\\\\wifi_7.data\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Dev\\nir\\_generated\\wifi_1.data 5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "D:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "D:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:3405: RuntimeWarning: Invalid value encountered in median\n",
      "  r = func(a, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Dev\\nir\\_generated\\wifi_2.data 5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "D:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "D:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:3405: RuntimeWarning: Invalid value encountered in median\n",
      "  r = func(a, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Dev\\nir\\_generated\\wifi_3.data 5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "D:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "D:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:3405: RuntimeWarning: Invalid value encountered in median\n",
      "  r = func(a, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "for freq in SAMPLING_FREQs:\n",
    "    wifi_pipeline(data_list, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# wifi_pipeline([\".\\\\wifi_23.data\"], SAMPLING_FREQ)#, \".\\\\user_1\\\\wifi_2.data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wifi_file_process(\".\\\\wifi_7.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}